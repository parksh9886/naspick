🏛️ Phase 1. 데이터 마이닝 (Data Mining)
목표: 백테스팅에 필요한 5년 치 재료(Raw Data)를 로컬 파일(CSV/Parquet)로 저장하기. (API 계속 호출하면 느리고 차단당하니까요.)

S&P 500 티커 및 섹터 확보:

Wikipedia 등에서 티커와 GICS Sector 정보를 긁어서 sp500_tickers.csv로 저장. (티커는 이미 보유하고있는듯. 불필요하면 패스)

주가 데이터 수집 (Bulk):

FinanceDataReader(fdr)를 사용하여 루프를 돌며 fdr.DataReader(ticker, '2020-01-01')로 개별 종목 데이터를 수집.

전체 종목의 수정 종가(Adj Close)와 거래량(Volume)을 다운받아 price_history.csv로 저장.

재무 데이터 수집 (Heavy Task):

여기가 가장 오래 걸립니다. 루프 돌면서 fdr에서 재무제표 데이터를 가져옵니다.

팁: 무료 소스 특성상 과거 특정 시점의 재무제표(Point-in-Time)를 구하기 어려우므로, 현재 기준 데이터를 가져오되 "과거에도 이 추세였다"고 가정하거나(단순화), 크롤링이 가능하다면 macrotrends 같은 곳에서 연도별 데이터를 긁어서 financials_history.csv로 저장합니다.

✅ 체크포인트: 로컬 폴더에 price.csv, financial.csv, sector.csv 3개 파일이 있으면 통과.

🧠 Phase 2. 랭킹 엔진 빌드 (Algorithm Build)
목표: 저장된 CSV를 읽어서, 2021년부터 오늘까지 **"모든 거래일의 종목별 랭킹"**이 적힌 마스터 테이블 만들기.

데이터 병합 (Merge):

주가 데이터와 재무 데이터를 날짜 기준으로 합칩니다.

재무 데이터는 분기별로 비어있으므로 ffill()(앞의 값으로 채우기)을 써서 매일매일 데이터가 있는 것처럼 만듭니다.

팩터 계산 (Feature Engineering):

모멘텀: 12개월, 6개월 수익률 계산.

밸류: PER, PBR 등 (음수는 NaN 처리 - v1.1 명세 적용).

랭킹 산출 (Scoring):

groupby(['Date', 'Sector'])를 적용하여 날짜별, 섹터별로 rank(pct=True) 실행.

v1.1 명세서의 가중치를 곱해서 Total_Score 산출.

최종 랭킹 확정:

점수 기준 내림차순 정렬하여 Rank 부여 (1위~500위).

✅ 체크포인트: df_rank_history라는 거대한 데이터프레임(약 100만 행)이 생성됨. 컬럼 예시: [Date, Ticker, Total_Score, Rank, Close_Price]

🧪 Phase 3. 백테스팅 시뮬레이션 (Backtesting)
목표: 마스터 테이블을 한 줄씩 읽으면서 가상의 계좌를 운영해보고 수익률 뽑기.

시뮬레이터 구현:

앞서 논의한 [전략 2: Top 10 진입 매수 / 20위 이탈 매도] 로직 구현.

for date in dates: 루프를 돌면서 매수/매도 실행.

자금 관리 로직 적용:

매수 시 현금이 부족하면 기존 종목 비중 축소(Re-weighting) 로직 적용.

최대 종목 수 20개 제한 등 버퍼 로직 확인.

벤치마크 비교:

같은 기간 SPY(S&P 500 ETF)를 샀을 때의 자산 변화도 같이 계산.

검증 (Validation):

결과 그래프가 너무 비정상적(수익률 10000% 등)이면 **미래 참조 오류(Look-ahead Bias)**가 없는지 코드 점검.

✅ 체크포인트: "연평균 수익률(CAGR) 15~25%" 정도의 현실적이면서도 우수한 그래프가 나오면 성공. portfolio_log.json 파일 생성.

💾 Phase 4. DB 적재 및 자동화 (Production Setup)
목표: 과거 데이터를 Supabase에 밀어 넣고, 내일부터는 자동으로 갱신되게 만들기.

Supabase 초기 세팅:

아까 알려드린 SQL 코드로 portfolio_log, portfolio_holdings 테이블 생성.

과거 데이터 업로드 (Initial Load):

Phase 3에서 만든 백테스팅 결과를 INSERT 쿼리로 한방에 업로드.

데일리 스크립트 작성 (daily_update.py):

이 스크립트는 **"오늘 하루"**에 대해서만 동작합니다.

데이터 수집(오늘자) → 랭킹 계산 → 포트폴리오 평가(보유종목 가격 반영) → 매매 판단 → DB 저장.

이 파일을 서버(혹은 내 PC) 스케줄러에 등록.

✅ 체크포인트: Supabase 대시보드에서 데이터가 예쁘게 들어가 있는 것을 확인.

🎨 Phase 5. 웹사이트 연동 (Frontend)
목표: 사용자가 볼 수 있게 차트 그리기.

API 호출:

프론트엔드 코드에서 supabase.from('portfolio_log').select('*') 호출.

차트 라이브러리 연동:

Recharts나 Chart.js 등에 데이터를 연결하여 선 그래프 그리기.

"S&P 500" 선은 회색, "내 알고리즘" 선은 강조색(파랑/빨강)으로 표시.

💡 꿀팁 (Pro Tips)
오늘 할 일: Phase 1, 2, 3까지만 하세요. 이것만 해도 시간이 꽤 걸립니다. DB 연결은 로직이 확실해진 다음에 해도 늦지 않습니다.

저장 습관: Phase 1에서 데이터를 다운로드했으면 반드시 CSV로 저장해두세요. 코드를 고칠 때마다 다시 다운로드하면 yfinance 속도 때문에 속 터집니다. (파일 읽는 건 1초면 됩니다.)

디버깅: 백테스팅 할 때 print 문을 찍어서 "왜 이 종목을 샀는지", "왜 팔았는지" 로그를 눈으로 확인해보세요. 의외로 엉뚱한 이유로 사고파는 경우가 많습니다.