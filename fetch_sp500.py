#!/usr/bin/env python3
"""
S&P 500 Stock Data Fetcher using FinanceDataReader
Updated: v1.1 Sector Relative Scoring
"""

import FinanceDataReader as fdr
import pandas as pd
import numpy as np
import json
import time
import os
import yfinance as yf
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Get S&P 500 list dynamically
def get_sp500_tickers():
    """Fetch latest S&P 500 list from FinanceDataReader"""
    try:
        sp500 = fdr.StockListing('SP500')
        return sp500['Symbol'].tolist()
    except:
        # Fallback to basic list if API fails
        return [
            "AAPL", "MSFT", "GOOGL", "AMZN", "NVDA", "META", "TSLA", "BRK.B", "UNH", "XOM",
            "JNJ", "JPM", "V", "PG", "MA", "HD", "CVX", "MRK", "ABBV", "PEP"
        ]

SP500_TICKERS = get_sp500_tickers()

# Ensure dual class shares and handle dot notation
REQUIRED_TICKERS = ['BRK.B', 'BF.B', 'GOOGL', 'GOOG', 'FOXA', 'FOX', 'NWSA', 'NWS']
for t in REQUIRED_TICKERS:
    # Remove hyphenated versions if present to standardise on dot
    t_hyphen = t.replace('.', '-')
    if t_hyphen in SP500_TICKERS:
        SP500_TICKERS.remove(t_hyphen)
    if t not in SP500_TICKERS:
        SP500_TICKERS.append(t)

# Remove duplicates
SP500_TICKERS = list(set(SP500_TICKERS))

# Map for fetching (Yahoo uses hyphen)
FETCH_MAP = {
    'BRK.B': 'BRK-B',
    'BF.B': 'BF-B'
}

print(f"âœ“ Loaded {len(SP500_TICKERS)} S&P 500 tickers (Adjusted for dual classes)")

# Import Korean names
# Try-catch to handle if file missing (though likely present)
try:
    from sp500_korean_names import SP500_KOREAN_NAMES
    STOCK_NAMES = SP500_KOREAN_NAMES
except ImportError:
    STOCK_NAMES = {}

SECTOR_MAP = {
    "Technology": "ê¸°ìˆ ", 
    "Information Technology": "ê¸°ìˆ ",
    "Communication Services": "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜",
    "Consumer Cyclical": "ì„ì˜ì†Œë¹„ì¬", 
    "Consumer Discretionary": "ì„ì˜ì†Œë¹„ì¬",
    "Consumer Defensive": "í•„ìˆ˜ì†Œë¹„ì¬",
    "Consumer Staples": "í•„ìˆ˜ì†Œë¹„ì¬",
    "Energy": "ì—ë„ˆì§€", 
    "Financial Services": "ê¸ˆìœµ", 
    "Financials": "ê¸ˆìœµ",
    "Financial": "ê¸ˆìœµ",
    "Healthcare": "í—¬ìŠ¤ì¼€ì–´", 
    "Health Care": "í—¬ìŠ¤ì¼€ì–´",
    "Industrials": "ì‚°ì—…ì¬",
    "Basic Materials": "ì†Œì¬", 
    "Materials": "ì†Œì¬",
    "Real Estate": "ë¶€ë™ì‚°", 
    "Utilities": "ìœ í‹¸ë¦¬í‹°"
}

# Fetch accurate sector data
def get_sector_data():
    try:
        sp500 = fdr.StockListing('SP500')
        sectors = dict(zip(sp500['Symbol'], sp500['Sector']))
        # Manual overrides
        sectors['BRK.B'] = 'Financials'
        sectors['BF.B'] = 'Consumer Staples'
        return sectors
    except:
        return {}

REAL_SECTORS = get_sector_data()

# Default sector assignments
TICKER_SECTORS = {
    "AAPL": "Technology", "MSFT": "Technology", "GOOGL": "Communication Services",
    "AMZN": "Consumer Cyclical", "NVDA": "Technology", "META": "Communication Services",
    "TSLA": "Consumer Cyclical", "JPM": "Financial", "V": "Financial",
    "MA": "Financial", "WMT": "Consumer Defensive", "JNJ": "Healthcare",
    "UNH": "Healthcare", "PFE": "Healthcare", "MRK": "Healthcare", "ABBV": "Healthcare",
    "XOM": "Energy", "CVX": "Energy", "KO": "Consumer Defensive", "PEP": "Consumer Defensive",
    "PG": "Consumer Defensive", "COST": "Consumer Defensive", "HD": "Consumer Cyclical",
    "NKE": "Consumer Cyclical", "MCD": "Consumer Cyclical", "SBUX": "Consumer Cyclical",
    "DIS": "Communication Services", "NFLX": "Communication Services", "CMCSA": "Communication Services",
    "VZ": "Communication Services", "AMD": "Technology", "INTC": "Technology",
    "NVDA": "Technology", "AVGO": "Technology", "CSCO": "Technology", "ORCL": "Technology",
    "ADBE": "Technology", "CRM": "Technology", "QCOM": "Technology", "TXN": "Technology",
    "ADBE": "Technology", "CRM": "Technology", "QCOM": "Technology", "TXN": "Technology",
}

# Fetch exchange data
def get_exchange_data():
    # print("  - Fetching exchange listings (NASDAQ, NYSE, AMEX)...")
    try:
        exchanges = {}
        # NASDAQ
        try:
            nasdaq = fdr.StockListing('NASDAQ')
            for t in nasdaq['Symbol']: exchanges[t] = 'NASDAQ'
        except: pass
        
        # NYSE
        try:
            nyse = fdr.StockListing('NYSE')
            for t in nyse['Symbol']: exchanges[t] = 'NYSE'
        except: pass

        # AMEX
        try:
            amex = fdr.StockListing('AMEX')
            for t in amex['Symbol']: exchanges[t] = 'AMEX'
        except: pass
        
        # Manual overrides
        exchanges['BRK.B'] = 'NYSE'
        exchanges['BF.B'] = 'NYSE'
        exchanges['DAY'] = 'NYSE' # Ensure DAY is correct
        
        return exchanges
    except Exception as e:
        # print(f"  âš  Exchange fetch failed: {e}")
        return {}

REAL_EXCHANGES = get_exchange_data()

def calculate_pivot_points(high, low, close):
    """Calculate pivot points"""
    pivot = (high + low + close) / 3
    r1 = (2 * pivot) - low
    r2 = pivot + (high - low)
    s1 = (2 * pivot) - high
    s2 = pivot - (high - low)
    
    return {
        "pivot": round(pivot, 2), "r1": round(r1, 2), "r2": round(r2, 2),
        "s1": round(s1, 2), "s2": round(s2, 2)
    }

def calculate_rsi(data, period=14):
    """Calculate RSI"""
    delta = data['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return rsi.iloc[-1] if len(rsi) > 0 else 50

def generate_ai_briefing(stats, levels, current_price, context):
    """Generate AI briefing with advanced logic"""
    briefing = []
    
    # 1. Trend Analysis (MACD & Moving Averages)
    if context.get('macd_golden'):
        briefing.append({"id": 1, "title": "MACD ê³¨ë“ í¬ë¡œìŠ¤",
            "text": "MACD ê³¨ë“ í¬ë¡œìŠ¤ê°€ ë°œìƒí•˜ì—¬ ë‹¨ê¸°ì ì¸ ìƒìŠ¹ ì¶”ì„¸ë¡œì˜ ì „í™˜ ì‹ í˜¸ê°€ í¬ì°©ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "color_class": "text-blue-400"})
    elif context.get('macd_dead'):
        briefing.append({"id": 1, "title": "MACD ë°ë“œí¬ë¡œìŠ¤",
            "text": "MACD ë°ë“œí¬ë¡œìŠ¤ ë°œìƒ, ë‹¨ê¸° ì¡°ì • ì••ë ¥ì´ ê±°ì„¸ì§ˆ ìˆ˜ ìˆì–´ ë¦¬ìŠ¤í¬ ê´€ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
            "color_class": "text-rose-400"})
    elif context.get('candle_hammer'):
         briefing.append({"id": 1, "title": "ë°”ë‹¥ê¶Œ ë§¤ìˆ˜ì„¸",
            "text": "í•˜ë½ì„¸ ëì—ì„œ ì €ì  ë§¤ìˆ˜ì„¸ê°€ ìœ ì…ë˜ëŠ” 'ë§ì¹˜í˜•' ìº”ë“¤ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë°”ë‹¥ ë‹¤ì§€ê¸°ë¥¼ ì‹œë„ ì¤‘ì…ë‹ˆë‹¤.",
            "color_class": "text-blue-400"})
    elif stats['trend'] >= 80:
        if stats['momentum'] >= 75: # Context: Strong but Overbought
             briefing.append({"id": 1, "title": "ê°•ë ¥í•œ ìƒìŠ¹ì„¸",
            "text": "íŒŒì£½ì§€ì„¸ì˜ ìƒìŠ¹ì„¸ë¥¼ ë³´ì´ê³  ìˆìœ¼ë‚˜, RSI ê³¼ì—´ê¶Œì— ì§„ì…í•˜ì—¬ ê±´ì „í•œ 'ìˆ¨ê³ ë¥´ê¸°' ì¡°ì •ì„ ì—¼ë‘ì— ë‘¬ì•¼ í•©ë‹ˆë‹¤.",
            "color_class": "text-blue-400"})
        else:
            briefing.append({"id": 1, "title": "ì´ìƒì  ì •ë°°ì—´",
                "text": "ì£¼ê°€ê°€ ëª¨ë“  ì´ë™í‰ê· ì„ (20, 60, 100) ìƒë‹¨ì— ìœ„ì¹˜í•˜ë©°, ê°€ì¥ ì´ìƒì ì´ê³  ê°•ë ¥í•œ ìƒìŠ¹ ì¶”ì„¸ë¥¼ ìœ ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
                "color_class": "text-blue-400"})
    elif stats['trend'] >= 50:
        briefing.append({"id": 1, "title": "ì¶”ì„¸ ì „í™˜ ì‹œë„",
            "text": "ë‹¨ê¸° í•˜ë½ì„¸ë¥¼ ë©ˆì¶”ê³  20ì¼ ì´í‰ì„ ì„ ëŒíŒŒí•˜ë©° ì˜ë¯¸ ìˆëŠ” ë°˜ë“± ì‹œê·¸ë„ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "color_class": "text-[#00bba3]"})
    else:
        briefing.append({"id": 1, "title": "ì¡°ì • êµ­ë©´",
            "text": "ì£¼ìš” ì§€ì§€ì„ ì„ ì´íƒˆí•˜ì—¬ ì•½ì„¸ê°€ ì§€ì† ì¤‘ì…ë‹ˆë‹¤. ì„£ë¶€ë¥¸ ì§„ì…ë³´ë‹¤ëŠ” ì§€ì§€ì„  í™•ì¸ì´ í•„ìš”í•œ ë³´ìˆ˜ì  êµ¬ê°„ì…ë‹ˆë‹¤.",
            "color_class": "text-gray-400"})
    
    # 2. Volume & Volatility (Bollinger & Volume)
    if context.get('bb_breakout'):
        briefing.append({"id": 2, "title": "ë³¼ë¦°ì € ë°´ë“œ ëŒíŒŒ",
            "text": "ë³¼ë¦°ì € ë°´ë“œ ìƒë‹¨ì„ ê°•í•˜ê²Œ ëŒíŒŒí•˜ë©° ì‹œì„¸ê°€ ë¶„ì¶œë˜ê³  ìˆìŠµë‹ˆë‹¤. ê°•ë ¥í•œ ëª¨ë©˜í…€ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "color_class": "text-blue-400"})
    elif context.get('bb_squeeze'):
        briefing.append({"id": 2, "title": "ë³€ë™ì„± ì¶•ì†Œ (Squeeze)",
            "text": "ë³€ë™ì„±ì´ ê·¹ë„ë¡œ ì¶•ì†Œëœ 'ìŠ¤í€´ì¦ˆ' êµ¬ê°„ì…ë‹ˆë‹¤. ì¡°ë§Œê°„ í° ë°©í–¥ì„±(ê¸‰ë“± ë˜ëŠ” ê¸‰ë½)ì´ ê²°ì •ë  ê²ƒì…ë‹ˆë‹¤.",
            "color_class": "text-[#00bba3]"})
    elif stats['volume'] >= 80:
        briefing.append({"id": 2, "title": "ìˆ˜ê¸‰ ì§‘ì¤‘",
            "text": "í‰ì†Œ ëŒ€ë¹„ 2ë°° ì´ìƒì˜ ëŒ€ëŸ‰ ê±°ë˜ëŸ‰ì´ í„°ì§€ë©° ë©”ì´ì € ì£¼ì²´(ê¸°ê´€/ì™¸ì¸)ì˜ ê°•ë ¥í•œ ê°œì…ì´ ì˜ì‹¬ë©ë‹ˆë‹¤.",
            "color_class": "text-[#00bba3]"})
    elif stats['volume'] <= 40:
        briefing.append({"id": 2, "title": "ê±°ë˜ëŸ‰ ì†Œê°•",
            "text": "ìƒìŠ¹ íƒ„ë ¥ì´ ë‘”í™”ë˜ë©° ê±°ë˜ëŸ‰ì´ ê°ì†Œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì‹œì¥ì˜ ê´€ì‹¬ì—ì„œ ë©€ì–´ì§„ ë°©í–¥ì„± íƒìƒ‰ êµ¬ê°„ì…ë‹ˆë‹¤.",
            "color_class": "text-gray-400"})
    else:
        briefing.append({"id": 2, "title": "ê²¬ì¡°í•œ ìˆ˜ê¸‰",
            "text": "íŠ¹ì´ ì‚¬í•­ ì—†ì´ ê¾¸ì¤€í•œ ê±°ë˜ëŸ‰ì„ ë™ë°˜í•˜ë©° í˜„ì¬ì˜ ì¶”ì„¸ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ë’·ë°›ì¹¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
            "color_class": "text-blue-400"})
    
    # 3. Strategy (Candle Patterns & Support/Resistance)
    if context.get('candle_shooting'):
        briefing.append({"id": 3, "title": "ê³ ì  ê²½ê³„ (ìœ ì„±í˜•)",
            "text": "ìƒìŠ¹ ì¶”ì„¸ ê³ ì ì—ì„œ ê¸´ ìœ—ê¼¬ë¦¬ë¥¼ ë‹¨ 'ìœ ì„±í˜•' ìº”ë“¤ì´ ê´€ì¸¡ë©ë‹ˆë‹¤. ì°¨ìµì‹¤í˜„ ë§¤ë¬¼ì„ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.",
            "color_class": "text-rose-400"})
    elif context.get('support_defense'):
        briefing.append({"id": 3, "title": "ì§€ì§€ì„  ë°©ì–´ ì„±ê³µ",
            "text": f"ì£¼ìš” ì§€ì§€ì„ ì¸ ${levels['s1']} ê°€ê²©ëŒ€ë¥¼ ì¥ì¤‘ í„°ì¹˜í–ˆìœ¼ë‚˜ ì§€ì¼œë‚´ë©° ì €ê°€ ë§¤ìˆ˜ì„¸ê°€ ì‚´ì•„ìˆìŒì„ ì¦ëª…í–ˆìŠµë‹ˆë‹¤.",
            "color_class": "text-blue-400"})
    elif stats['momentum'] >= 75:
        briefing.append({"id": 3, "title": "ê³¼ì—´ê¶Œ ì§„ì…",
            "text": f"RSI ê³¼ì—´ê¶Œì— ì§„ì…í–ˆìŠµë‹ˆë‹¤. ì¶”ê²© ë§¤ìˆ˜ë³´ë‹¤ëŠ” ëˆŒë¦¼ëª©(${levels['s1']}) ì§€ì§€ë¥¼ í™•ì¸í•  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ëŠ” ê²ƒì´ ìœ ë¦¬í•©ë‹ˆë‹¤.",
            "color_class": "text-rose-400"})
    elif stats['momentum'] <= 25:
        briefing.append({"id": 3, "title": "ê¸°ìˆ ì  ë°˜ë“± ê¸°ëŒ€",
            "text": "ê³¼ë§¤ë„(ì¹¨ì²´) êµ¬ê°„ì— ì§„ì…í–ˆìŠµë‹ˆë‹¤. ë‹¨ê¸° ë‚™í­ ê³¼ëŒ€ë¡œ ì¸í•œ ê¸°ìˆ ì  ë°˜ë“±(Dead Cat Bounce)ì´ ê¸°ëŒ€ë˜ëŠ” ìœ„ì¹˜ì…ë‹ˆë‹¤.",
            "color_class": "text-blue-400"})
    else:
        briefing.append({"id": 3, "title": "í™€ë”© ì „ëµ",
            "text": f"í˜„ì¬ ì¶”ì„¸ê°€ ìœ íš¨í•˜ë¯€ë¡œ 1ì°¨ ì§€ì§€ì„ (${levels['s1']})ì„ ì´íƒˆí•˜ì§€ ì•ŠëŠ” í•œ ì¶”ì„¸ ì¶”ì¢…(Trend Following) ì „ëµì„ ê¶Œì¥í•©ë‹ˆë‹¤.",
            "color_class": "text-gray-400"})
    
    return briefing


# --------------------------------------------------------------------------------
# NEW SCORING ENGINE (v1.1 - Sector Relative + Financials)
# --------------------------------------------------------------------------------

def calculate_technical_factors_bulk(df):
    """Bulk calculation of technical factors for all tickers"""
    print("ğŸ“ˆ Calculating Technical Factors (Momentum & Vol)...")
    
    # Ensure sorted
    df = df.sort_values(['Ticker', 'Date'])
    
    # Returns (Momentum)
    # Using 'fill_method=None' to avoid future warnings
    df['Return_12M'] = df.groupby('Ticker')['Close'].pct_change(periods=252, fill_method=None)
    df['Return_6M'] = df.groupby('Ticker')['Close'].pct_change(periods=126, fill_method=None)
    df['Return_3M'] = df.groupby('Ticker')['Close'].pct_change(periods=63, fill_method=None)
    
    # Volume Spike: Current Vol / 3M Avg Vol
    df['Vol_3M_Avg'] = df.groupby('Ticker')['Volume'].transform(lambda x: x.rolling(window=63).mean())
    df['Vol_Spike'] = df['Volume'] / df['Vol_3M_Avg']
    
    return df

def apply_masks_and_score_bulk(daily_df, financial_map):
    """
    Apply Sector Relative Scoring (The Core Logic)
    """
    # Merge financials
    merged = daily_df.merge(financial_map, on='Ticker', how='left')
    
    # Filter unknown sectors
    merged = merged.dropna(subset=['Sector'])
    
    # 1. Mask Negative Valuations (Give them NaN so they rank poorly or handled later)
    # Actually for "Low is Good", if we fillna with high number, it ranks bad.
    # Logic in engine.py: mask negative to NaN.
    val_cols = ['PER', 'PBR', 'EV_EBITDA', 'PSR']
    for col in val_cols:
        if col in merged.columns:
            merged[col] = merged[col].apply(lambda x: x if x > 0 else np.nan)
            
    # 2. Sector Relative Ranking (0.0 to 1.0)
    
    # A. Value (25 pts) - Low is Good
    # Note: NaN values in 'PER' (negatives) will be ranked?
    # Pandas rank handles NaN: assigns NaN. We fillna(0) score later.
    merged['Score_PER'] = (1 - merged.groupby('Sector')['PER'].rank(pct=True, ascending=True)) * 10
    merged['Score_PBR'] = (1 - merged.groupby('Sector')['PBR'].rank(pct=True, ascending=True)) * 5
    merged['Score_PSR'] = (1 - merged.groupby('Sector')['PSR'].rank(pct=True, ascending=True)) * 5
    merged['Score_EVEB'] = (1 - merged.groupby('Sector')['EV_EBITDA'].rank(pct=True, ascending=True)) * 5
    
    # B. Growth (25 pts) - High is Good
    merged['Score_RevG'] = merged.groupby('Sector')['Rev_Growth'].rank(pct=True, ascending=True) * 10
    # Boost EPS weight as per engine logic
    merged['Score_EPSG'] = merged.groupby('Sector')['EPS_Growth'].rank(pct=True, ascending=True) * 15
    
    # C. Profitability (20 pts) - High is Good
    merged['Score_ROE'] = merged.groupby('Sector')['ROE'].rank(pct=True, ascending=True) * 10
    # Map Profit_Margin/Oper_Margin if columns exist
    if 'Profit_Margin' in merged.columns:
        merged['Score_NM'] = merged.groupby('Sector')['Profit_Margin'].rank(pct=True, ascending=True) * 5
    else:
        merged['Score_NM'] = 0
        
    if 'Oper_Margin' in merged.columns:
        merged['Score_OM'] = merged.groupby('Sector')['Oper_Margin'].rank(pct=True, ascending=True) * 5
    else:
        merged['Score_OM'] = 0
        
    # D. Momentum (20 pts) - High is Good
    merged['Score_Mom1Y'] = merged.groupby('Sector')['Return_12M'].rank(pct=True, ascending=True) * 10
    merged['Score_Mom6M'] = merged.groupby('Sector')['Return_6M'].rank(pct=True, ascending=True) * 5
    merged['Score_Mom3M'] = merged.groupby('Sector')['Return_3M'].rank(pct=True, ascending=True) * 5
    
    # E. Sentiment (10 pts) - High is Good (Volume Spike)
    merged['Score_Vol'] = merged.groupby('Sector')['Vol_Spike'].rank(pct=True, ascending=True) * 10
    
    # 3. Fill NaNs with 0 (Penalty for missing/negative data)
    score_cols = [c for c in merged.columns if c.startswith('Score_')]
    merged[score_cols] = merged[score_cols].fillna(0)
    
    # 4. Total Score
    merged['Total_Score'] = merged[score_cols].sum(axis=1)
    
    # Final Rank
    merged['Rank'] = merged['Total_Score'].rank(ascending=False, method='min')
    
    return merged

def analyze_single_stock_context(ticker, hist, final_score_row):
    """
    Generate Technical Context and Briefing for a single stock
    (Used for the UI popup)
    """
    try:
        current_price = hist['Close'].iloc[-1]
        prev_close = hist['Close'].iloc[-2]
        
        # Pivot points
        yest_high = hist['High'].iloc[-2]
        yest_low = hist['Low'].iloc[-2]
        yest_close = hist['Close'].iloc[-2]
        levels = calculate_pivot_points(yest_high, yest_low, yest_close)
        
        # Moving averages
        sma20 = hist['Close'].rolling(window=20).mean().iloc[-1]
        
        # Bollinger Bands
        sma20_series = hist['Close'].rolling(window=20).mean()
        std20_series = hist['Close'].rolling(window=20).std()
        upper_band = (sma20_series + (std20_series * 2)).iloc[-1]
        lower_band = (sma20_series - (std20_series * 2)).iloc[-1]
        
        # Bandwidth squeeze
        bandwidth = (upper_band - lower_band) / sma20_series.iloc[-1]
        past_bandwidth = ((sma20_series + (std20_series * 2)) - (sma20_series - (std20_series * 2))) / sma20_series
        is_squeeze = bandwidth <= past_bandwidth.rolling(window=20).min().iloc[-1]

        # RSI
        rsi = calculate_rsi(hist)
        
        # MACD
        exp12 = hist['Close'].ewm(span=12, adjust=False).mean()
        exp26 = hist['Close'].ewm(span=26, adjust=False).mean()
        macd = exp12 - exp26
        signal = macd.ewm(span=9, adjust=False).mean()
        macd_val = macd.iloc[-1]
        signal_val = signal.iloc[-1]
        
        macd_prev = macd.iloc[-2]
        signal_prev = signal.iloc[-2]
        macd_golden = (macd_prev < signal_prev) and (macd_val > signal_val)
        macd_dead = (macd_prev > signal_prev) and (macd_val < signal_val)
        
        # Candle Patterns
        open_p = hist['Open'].iloc[-1]
        close_p = hist['Close'].iloc[-1]
        high_p = hist['High'].iloc[-1]
        low_p = hist['Low'].iloc[-1]
        body = abs(close_p - open_p)
        upper_shadow = high_p - max(open_p, close_p)
        lower_shadow = min(open_p, close_p) - low_p
        
        is_hammer = (current_price < sma20) and (lower_shadow > body * 2) and (upper_shadow < body * 0.5)
        is_shooting = (current_price > sma20) and (upper_shadow > body * 2) and (lower_shadow < body * 0.5)
        is_support_defense = (low_p <= levels['s1']) and (close_p > levels['s1'])

        context = {
            "macd_golden": macd_golden,
            "macd_dead": macd_dead,
            "bb_breakout": (current_price > upper_band) and (rsi < 75),
            "bb_squeeze": is_squeeze,
            "candle_hammer": is_hammer,
            "candle_shooting": is_shooting,
            "support_defense": is_support_defense
        }
        
        # Signals list for UI
        signals = []
        if rsi > 70: signals.append("RSI_Overbought")
        elif rsi < 30: signals.append("RSI_Oversold")
        if macd_golden: signals.append("MACD_GoldenCross")
        if is_squeeze: signals.append("Vol_Squeeze")
        
        # Create Stats Bar from Score (Normalized to 0-100)
        # Value = PER + PBR + PSR + EVEB (Max 25)
        val_score = final_score_row['Score_PER'] + final_score_row['Score_PBR'] + final_score_row['Score_PSR'] + final_score_row['Score_EVEB']
        # Growth = Rev + EPS (Max 25)
        growth_score = final_score_row['Score_RevG'] + final_score_row['Score_EPSG']
        # Profit = ROE + NM + OM (Max 20)
        prof_score = final_score_row['Score_ROE'] + final_score_row['Score_NM'] + final_score_row['Score_OM']
        # Momentum = 1Y + 6M + 3M (Max 20)
        mom_score = final_score_row['Score_Mom1Y'] + final_score_row['Score_Mom6M'] + final_score_row['Score_Mom3M']
        
        stats_bar = {
            "trend": 0, # Placeholder or map from Growth?
            "volume": int(final_score_row['Score_Vol'] * 10), # Vol Score is max 10, scale to 100?
            "momentum": int(mom_score * 5),
            "impact": int(val_score * 4) # Value score max 25 -> 100
        }
        # Mapping "Trend" to Growth/Profit combination?
        stats_bar['trend'] = int((growth_score + prof_score) / 45 * 100)
        
        # Generate Briefing
        ai_briefing = generate_ai_briefing(stats_bar, levels, current_price, context)
        
        return {
            "stats_bar": stats_bar,
            "signals": signals,
            "levels": levels,
            "ai_briefing": ai_briefing,
            "current_price": current_price,
            "change_pct": (current_price - prev_close)/prev_close * 100
        }
        
    except Exception as e:
        # print(f"Error context {ticker}: {e}")
        # Return fallback
        return None

def get_market_caps_bulk(tickers):
    """
    Fetch market cap for all tickers using yfinance .info (threaded).
    Returns: Dict {ticker: market_cap}
    """
    print(f"ğŸ’° Fetching Market Caps for {len(tickers)} tickers via yfinance...")
    mcaps = {}
    
    import concurrent.futures
    
    def fetch_mcap(t):
        try:
            yf_t = t.replace('.', '-')
            if t == 'BRK.B': yf_t = 'BRK-B'
            return t, yf.Ticker(yf_t).info.get('marketCap', 0)
        except:
            return t, 0

    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
        futures = {executor.submit(fetch_mcap, t): t for t in tickers}
        for i, future in enumerate(concurrent.futures.as_completed(futures)):
            t, mcap = future.result()
            mcaps[t] = mcap
            if i % 50 == 0:
                print(f"   Getting Market Caps... [{i}/{len(tickers)}]", end='\r')
            
    print("\nâœ“ Market Cap fetch complete.")
    return mcaps

def main():
    print("ğŸš€ Fetching S&P 500 Stock Data (v1.1 Sector Relative Logic)")
    print(f"â° Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # 1. Load Financials
    print("ğŸ“‚ Loading Financial Data...")
    try:
        if not os.path.exists('data/financials.csv'):
            print("âŒ data/financials.csv not found! Please run mining script first.")
            return
        df_fin = pd.read_csv('data/financials.csv')
        print(f"âœ“ Loaded {len(df_fin)} financial records")
    except Exception as e:
        print(f"âŒ Error loading financials: {e}")
        return

    # 2. Fetch Price History (Bulk)
    print(f"ğŸ“Š Fetching Price Data for {len(SP500_TICKERS)} tickers...")
    
    end_date = datetime.now()
    start_date = end_date - timedelta(days=400) # Need > 1 year for 12M Return
    
    all_hist_list = []
    
    # Using TQDM if available, else standard print
    for idx, ticker in enumerate(SP500_TICKERS, 1):
        try:
            if idx % 50 == 0: print(f"   [{idx}/{len(SP500_TICKERS)}] Fetched...")
            
            fetch_ticker = FETCH_MAP.get(ticker, ticker)
            hist = fdr.DataReader(fetch_ticker, start_date, end_date)
            
            if hist.empty or len(hist) < 260: # Need at least ~1 year
                continue
                
            hist['Ticker'] = ticker
            # Keep only necessary columns to save memory
            hist = hist[['Ticker', 'Open', 'High', 'Low', 'Close', 'Volume']]
            hist.index.name = 'Date'
            hist = hist.reset_index()
            
            all_hist_list.append(hist)
            
        except Exception as e:
            continue
            
    if not all_hist_list:
        print("âŒ No price data fetched.")
        return
        
    df_all_price = pd.concat(all_hist_list)
    print(f"âœ“ Fetched {len(df_all_price)} total rows.")
    
    # 3. Calculate Technicals (Bulk)
    df_all_price = calculate_technical_factors_bulk(df_all_price)
    
    # 4. Score Logic (Sector Relative) - Only for Latest Date
    print("ğŸ† calculating Scores (Sector Ranking)...")
    latest_date = df_all_price['Date'].max()
    # Find the max date that covers most stocks?
    # Some stocks might have data for today, some yesterday.
    # Safe bet: Take Max Date.
    print(f"   Target Date: {latest_date.date()}")
    
    df_latest = df_all_price[df_all_price['Date'] == latest_date].copy()
    
    ranked_df = apply_masks_and_score_bulk(df_latest, df_fin)
    
    # 5. Fetch Market Caps (New Step)
    market_caps = get_market_caps_bulk(ranked_df['Ticker'].tolist())
    
    # 6. Generate Final JSON Output
    print("ğŸ“ Generating Final JSON...")
    
    # Load Yesterday's ranks
    yesterday_ranks = {}
    if os.path.exists('yesterday_ranks.json'):
         with open('yesterday_ranks.json', 'r', encoding='utf-8') as f:
            yesterday_ranks = json.load(f)

    final_results = []
    
    # Optimization: Create a dict of dataframes for fast history lookup
    ticker_dfs = {x: y for x, y in df_all_price.groupby('Ticker')}
    
    for idx, row in ranked_df.iterrows():
        ticker = row['Ticker']
        hist = ticker_dfs.get(ticker)
        
        if hist is None: continue
        
        # Generate Context
        ctx = analyze_single_stock_context(ticker, hist, row)
        if not ctx: continue
        
        # Metadata
        name = STOCK_NAMES.get(ticker, ticker)
        
        # Use Sector Map to get Korean Name
        raw_sector = row['Sector']
        sector_kr = SECTOR_MAP.get(raw_sector, raw_sector)
        exchange = REAL_EXCHANGES.get(ticker, "NASDAQ")
        
        # Rank Changes
        current_rank = int(row['Rank'])
        prev_info = yesterday_ranks.get(ticker, {})
        prev_rank = prev_info.get('rank', 0)
        rank_change = (prev_rank - current_rank) if prev_rank else 0
        
        # Build Object
        item = {
            "ticker": ticker,
            "name": name,
            "name_en": ticker,
            "exchange": exchange,
            "sector": sector_kr,
            "current_price": round(ctx['current_price'], 2),
            "change_pct": round(ctx['change_pct'], 2),
            "market_cap": market_caps.get(ticker, 0), # Populated from yfinance
            "base_score": 0, # Deprecated in v1.1, used placeholder
            "bonus_score": 0, # Deprecated
            "final_score": round(row['Total_Score'], 1),
            "rank": current_rank,
            "rank_change": rank_change,
            "stats_bar": ctx['stats_bar'],
            "signals": ctx['signals'],
            "levels": ctx['levels'],
            "ai_briefing": ctx['ai_briefing'],
            "related_peers": [] # Fill later
        }
        
        # Tier logic
        # Rank is 1-based.
        total = len(ranked_df)
        pct = current_rank / total
        if pct <= 0.01: item['tier'] = "OP"
        elif pct <= 0.05: item['tier'] = 1
        elif pct <= 0.20: item['tier'] = 2
        elif pct <= 0.50: item['tier'] = 3
        elif pct <= 0.80: item['tier'] = 4
        else: item['tier'] = 5
        
        final_results.append(item)
        
    # Sort by Rank
    final_results.sort(key=lambda x: x['rank'])
    
    # 7. Sector Ranking & Peers
    # Group by sector
    by_sector = {}
    for item in final_results:
        sec = item['sector']
        if sec not in by_sector: by_sector[sec] = []
        by_sector[sec].append(item)
        
    for sec, items in by_sector.items():
        # Items sorted by rank (since final_results was sorted)
        for s_idx, item in enumerate(items):
            item['sector_rank'] = s_idx + 1
            
            # Related peers (Top 3 in same sector, excluding self)
            peers = [p for p in items if p['ticker'] != item['ticker']][:3]
            item['related_peers'] = [{"ticker": p['ticker'], "change_pct": p['change_pct']} for p in peers]
            
    # Save
    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False)
        
    print(f"\nâœ… Success! Saved {len(final_results)} stocks to data.json")

    # Save current ranks for tomorrow
    # Create simple dict {ticker: {rank, sector_rank}}
    snapshot = {}
    for item in final_results:
        snapshot[item['ticker']] = {
            "rank": item['rank'],
            "sector_rank": item.get('sector_rank', 0)
        }
    with open('yesterday_ranks.json', 'w', encoding='utf-8') as f:
        json.dump(snapshot, f, indent=2)

if __name__ == "__main__":
    main()
